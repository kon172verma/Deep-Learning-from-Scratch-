{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwJUJ2VLBYJF",
        "colab_type": "text"
      },
      "source": [
        "<h2><b>Logistic Regression using Numpy</b></h2>\n",
        "<p>-Konark Verma\n",
        "<p>We wish to implement a single neuron Neural Network which is same as logistic regression and test this on a given dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9CO9FwTvqig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the required libraries.\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZNV69EdwAsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining functions for sigmoid, loss, accuracy, initializing parameters, forward and backward propogation.\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def initialize_parameters(n):\n",
        "    w = np.zeros((n,1))\n",
        "    b = np.zeros((1,1))\n",
        "    return w, b\n",
        "\n",
        "def compute_loss(Y, a, m):\n",
        "    loss = -np.sum(Y * np.log(a) + (1-Y) * np.log(1-a))/m\n",
        "    return np.round(loss,3)\n",
        "\n",
        "def compute_accuracy(Y, a, m):\n",
        "    a = (a+0.5).astype(int)\n",
        "    accuracy = 1 - np.sum(a^Y.astype(int))/m\n",
        "    return np.round(accuracy*100,2)\n",
        "\n",
        "def forward_propogation(X, Y, w, b, m):\n",
        "    z = np.dot(w.T,X) + b\n",
        "    a = sigmoid(z)\n",
        "    loss = compute_loss(Y, a, m)\n",
        "    accuracy = compute_accuracy(Y, a, m)\n",
        "    return a, loss, accuracy\n",
        "\n",
        "def compute_gradients(X, Y, a, m):\n",
        "    dz = a-Y\n",
        "    dw = np.dot(X, dz.T)/m\n",
        "    db = np.sum(dz)/m\n",
        "    return dw, db\n",
        "\n",
        "def backward_propogation(X, Y, a, m, w, b, learning_rate):\n",
        "    dw, db = compute_gradients(X, Y, a, m)\n",
        "    w = w - learning_rate * dw\n",
        "    b = b - learning_rate * db\n",
        "    return w, b"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP4M3l7BzMLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining function for Logistic Regression.\n",
        "def logistic_regression(X, Y, X_test, Y_test, epochs=100, learning_rate=1):\n",
        "    n, m = X.shape\n",
        "    w, b = initialize_parameters(n)\n",
        "    print('\\nTraining Logistic Regression\\n')\n",
        "    for epoch in range(epochs):\n",
        "        a, loss, accuracy = forward_propogation(X, Y, w, b, m)\n",
        "        print('\\tEpoch :',epoch+1, 'Loss :',loss, 'Accuracy :',accuracy)\n",
        "        w, b = backward_propogation(X, Y, a, m, w, b, learning_rate)\n",
        "    print('\\nTesting Logistic Regression\\n')\n",
        "    a, loss, accuracy = forward_propogation(X_test, Y_test, w, b, X_test.shape[1])\n",
        "    print('\\tLoss :',loss, 'Accuracy :',accuracy)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCzrjxn-1shB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d09f288a-6da5-4e73-9979-596e2c44284e"
      },
      "source": [
        "# Preprocessing the training and testing data.\n",
        "data = np.genfromtxt('Dataset.csv', delimiter=',')\n",
        "data_x, data_y = preprocessing.scale(data[:,:5]), data[:,5]\n",
        "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.3)\n",
        "train_x, train_y, test_x, test_y = train_x.T, train_y.reshape(1,len(train_y)), test_x.T, test_y.reshape(1,len(test_y))\n",
        "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5, 700), (1, 700), (5, 300), (1, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocKaC3Tb6mcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "1c0941ae-3733-44b4-dbb6-2978f4440c49"
      },
      "source": [
        "# Training and testing the logistic regression model.\n",
        "logistic_regression(train_x, train_y, test_x, test_y, epochs=50, learning_rate=10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Logistic Regression\n",
            "\n",
            "\tEpoch : 1 Loss : 0.693 Accuracy : 58.14\n",
            "\tEpoch : 2 Loss : 0.269 Accuracy : 91.57\n",
            "\tEpoch : 3 Loss : 0.259 Accuracy : 94.57\n",
            "\tEpoch : 4 Loss : 0.259 Accuracy : 94.86\n",
            "\tEpoch : 5 Loss : 0.259 Accuracy : 94.86\n",
            "\tEpoch : 6 Loss : 0.259 Accuracy : 94.86\n",
            "\tEpoch : 7 Loss : 0.259 Accuracy : 94.86\n",
            "\tEpoch : 8 Loss : 0.259 Accuracy : 94.86\n",
            "\tEpoch : 9 Loss : 0.259 Accuracy : 94.86\n",
            "\tEpoch : 10 Loss : 0.259 Accuracy : 94.86\n",
            "\tEpoch : 11 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 12 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 13 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 14 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 15 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 16 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 17 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 18 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 19 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 20 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 21 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 22 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 23 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 24 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 25 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 26 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 27 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 28 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 29 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 30 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 31 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 32 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 33 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 34 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 35 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 36 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 37 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 38 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 39 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 40 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 41 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 42 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 43 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 44 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 45 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 46 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 47 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 48 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 49 Loss : 0.259 Accuracy : 95.0\n",
            "\tEpoch : 50 Loss : 0.259 Accuracy : 95.0\n",
            "\n",
            "Testing Logistic Regression\n",
            "\n",
            "\tLoss : 0.277 Accuracy : 95.33\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}