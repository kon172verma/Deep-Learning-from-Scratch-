{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGP0CPnK4qrh",
        "colab_type": "text"
      },
      "source": [
        "<h2><b>Multi Hidden Layer Neural Network or Deep Neural Network using Numpy</b></h2>\n",
        "<p>-Konark Verma\n",
        "<p>We wish to implement a Multi Hidden Layer Neural Network, from scratch just using Numpy and test this on a given dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIRCBYUA_Azk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the required libraries.\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeX7flnMg6Tb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a43d05df-a8ce-4e14-c49e-a44193def440"
      },
      "source": [
        "# Preprocessing the training and testing data.\n",
        "data = np.genfromtxt('Dataset.csv', delimiter=',')\n",
        "data_x, data_y = preprocessing.scale(data[:,:5]), data[:,5]\n",
        "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.3, random_state=1)\n",
        "train_x, train_y, test_x, test_y = train_x.T, train_y.reshape(1,len(train_y)), test_x.T, test_y.reshape(1,len(test_y))\n",
        "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5, 700), (1, 700), (5, 300), (1, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1d_Zz0wUTAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for sigmoid, relu, initializing parameters, computing loss and accuracy.\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.where(z>0, z, 0)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return np.where(z>0, 1, 0)\n",
        "\n",
        "def initialize_parameters(n, hidden_layers):\n",
        "    weights, biases = [], []\n",
        "    temp = [n]+hidden_layers+[1]\n",
        "    for i in range(1, len(temp)):\n",
        "        weights.append(np.random.rand(temp[i], temp[i-1])*0.01)\n",
        "        biases.append(np.zeros((temp[i],1)))\n",
        "    return weights, biases\n",
        "\n",
        "def compute_loss(Y, a, m):\n",
        "    loss = -np.sum(Y*np.log(a) + (1-Y)*np.log(1-a))/m\n",
        "    return np.round(loss,3)\n",
        "\n",
        "def compute_accuracy(Y, a, m):\n",
        "    accuracy = 1 - np.sum((a+0.5).astype(int) ^ Y.astype(int))/m\n",
        "    return np.round(accuracy*100,2)\n",
        "    \n",
        "def forward_propogation(X, Y, weights, biases):\n",
        "    m = X.shape[1]\n",
        "    caches = []\n",
        "    a_prev = X\n",
        "    for i in range(len(weights)-1):\n",
        "        z = np.dot(weights[i], a_prev) + biases[i]\n",
        "        a = relu(z)\n",
        "        cache = [weights[i+1], z, a_prev]\n",
        "        caches.append(cache)\n",
        "        a_prev = a\n",
        "    z = np.dot(weights[-1], a_prev) + biases[-1]\n",
        "    a = sigmoid(z)\n",
        "    cache = [a_prev]\n",
        "    caches.append(cache)\n",
        "    loss = compute_loss(Y, a, m)\n",
        "    accuracy = compute_accuracy(Y, a, m)\n",
        "    return a, caches, loss, accuracy\n",
        "\n",
        "def backward_propogation(X, Y, weights, biases, a, caches, learning_rate):\n",
        "    m = X.shape[1]\n",
        "    dz = a - Y\n",
        "    dw = np.dot(dz, caches[-1][0].T)/m\n",
        "    db = np.sum(dz, axis=1, keepdims=True)/m\n",
        "    weights_derivative, biases_derivative = [dw], [db]\n",
        "    for i in range(len(weights)-2,-1,-1):\n",
        "        w_next, z, a_prev = caches[i]\n",
        "        dz = np.dot(w_next.T, dz) * relu_derivative(z)\n",
        "        dw = np.dot(dz, a_prev.T)/m\n",
        "        db = np.sum(dz, axis=1, keepdims=True)/m\n",
        "        weights_derivative.append(dw)\n",
        "        biases_derivative.append(db)\n",
        "    weights_derivative.reverse()\n",
        "    biases_derivative.reverse()\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] = weights[i] - learning_rate*weights_derivative[i]\n",
        "        biases[i] = biases[i] - learning_rate*biases_derivative[i]\n",
        "    return weights, biases"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Kpl6aJrBJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for our multi layer neural network.\n",
        "def deep_neural_network(X, Y, X_test, Y_test, hidden_units, learning_rate=1, epochs=50):\n",
        "    n, m = X.shape\n",
        "    weights, biases = initialize_parameters(n, hidden_units)\n",
        "    print('\\nTraining our Deep Neural Network:')\n",
        "    for epoch in range(epochs):\n",
        "        a, cache, loss, accuracy = forward_propogation(X, Y, weights, biases)\n",
        "        print('\\tEpoch :',epoch+1, '\\tLoss:',loss, '\\tAccuracy:',accuracy)\n",
        "        weights, biases = backward_propogation(X, Y, weights, biases, a, cache, learning_rate)\n",
        "    print('\\nTesting our Deep Neural Network:')\n",
        "    _, _, loss, accuracy = forward_propogation(X_test, Y_test, weights, biases)\n",
        "    print('\\tLoss:',loss, '\\tAccuracy:',accuracy)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwWf89p6hLgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1fe38db-775a-4424-ba12-f4ae84439dc7"
      },
      "source": [
        "# Training and testing our neural network on our dataset.\n",
        "hidden_units = [128,32,8]\n",
        "deep_neural_network(train_x, train_y, test_x, test_y, hidden_units, learning_rate=1, epochs=500)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training our Deep Neural Network:\n",
            "\tEpoch : 1 \tLoss: 0.693 \tAccuracy: 58.86\n",
            "\tEpoch : 2 \tLoss: 0.686 \tAccuracy: 58.86\n",
            "\tEpoch : 3 \tLoss: 0.682 \tAccuracy: 58.86\n",
            "\tEpoch : 4 \tLoss: 0.68 \tAccuracy: 58.86\n",
            "\tEpoch : 5 \tLoss: 0.679 \tAccuracy: 58.86\n",
            "\tEpoch : 6 \tLoss: 0.678 \tAccuracy: 58.86\n",
            "\tEpoch : 7 \tLoss: 0.678 \tAccuracy: 58.86\n",
            "\tEpoch : 8 \tLoss: 0.678 \tAccuracy: 58.86\n",
            "\tEpoch : 9 \tLoss: 0.678 \tAccuracy: 58.86\n",
            "\tEpoch : 10 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 11 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 12 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 13 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 14 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 15 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 16 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 17 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 18 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 19 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 20 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 21 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 22 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 23 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 24 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 25 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 26 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 27 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 28 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 29 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 30 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 31 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 32 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 33 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 34 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 35 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 36 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 37 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 38 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 39 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 40 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 41 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 42 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 43 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 44 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 45 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 46 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 47 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 48 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 49 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 50 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 51 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 52 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 53 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 54 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 55 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 56 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 57 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 58 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 59 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 60 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 61 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 62 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 63 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 64 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 65 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 66 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 67 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 68 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 69 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 70 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 71 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 72 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 73 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 74 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 75 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 76 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 77 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 78 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 79 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 80 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 81 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 82 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 83 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 84 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 85 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 86 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 87 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 88 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 89 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 90 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 91 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 92 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 93 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 94 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 95 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 96 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 97 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 98 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 99 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 100 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 101 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 102 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 103 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 104 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 105 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 106 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 107 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 108 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 109 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 110 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 111 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 112 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 113 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 114 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 115 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 116 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 117 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 118 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 119 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 120 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 121 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 122 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 123 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 124 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 125 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 126 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 127 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 128 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 129 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 130 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 131 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 132 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 133 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 134 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 135 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 136 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 137 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 138 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 139 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 140 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 141 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 142 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 143 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 144 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 145 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 146 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 147 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 148 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 149 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 150 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 151 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 152 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 153 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 154 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 155 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 156 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 157 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 158 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 159 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 160 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 161 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 162 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 163 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 164 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 165 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 166 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 167 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 168 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 169 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 170 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 171 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 172 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 173 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 174 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 175 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 176 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 177 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 178 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 179 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 180 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 181 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 182 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 183 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 184 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 185 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 186 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 187 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 188 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 189 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 190 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 191 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 192 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 193 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 194 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 195 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 196 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 197 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 198 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 199 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 200 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 201 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 202 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 203 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 204 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 205 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 206 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 207 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 208 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 209 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 210 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 211 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 212 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 213 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 214 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 215 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 216 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 217 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 218 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 219 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 220 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 221 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 222 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 223 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 224 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 225 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 226 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 227 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 228 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 229 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 230 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 231 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 232 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 233 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 234 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 235 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 236 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 237 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 238 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 239 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 240 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 241 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 242 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 243 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 244 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 245 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 246 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 247 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 248 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 249 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 250 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 251 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 252 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 253 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 254 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 255 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 256 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 257 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 258 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 259 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 260 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 261 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 262 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 263 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 264 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 265 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 266 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 267 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 268 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 269 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 270 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 271 \tLoss: 0.677 \tAccuracy: 58.86\n",
            "\tEpoch : 272 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 273 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 274 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 275 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 276 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 277 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 278 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 279 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 280 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 281 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 282 \tLoss: 0.676 \tAccuracy: 58.86\n",
            "\tEpoch : 283 \tLoss: 0.675 \tAccuracy: 58.86\n",
            "\tEpoch : 284 \tLoss: 0.675 \tAccuracy: 58.86\n",
            "\tEpoch : 285 \tLoss: 0.675 \tAccuracy: 58.86\n",
            "\tEpoch : 286 \tLoss: 0.675 \tAccuracy: 58.86\n",
            "\tEpoch : 287 \tLoss: 0.674 \tAccuracy: 58.86\n",
            "\tEpoch : 288 \tLoss: 0.674 \tAccuracy: 58.86\n",
            "\tEpoch : 289 \tLoss: 0.674 \tAccuracy: 58.86\n",
            "\tEpoch : 290 \tLoss: 0.673 \tAccuracy: 58.86\n",
            "\tEpoch : 291 \tLoss: 0.673 \tAccuracy: 58.86\n",
            "\tEpoch : 292 \tLoss: 0.672 \tAccuracy: 58.86\n",
            "\tEpoch : 293 \tLoss: 0.671 \tAccuracy: 58.86\n",
            "\tEpoch : 294 \tLoss: 0.67 \tAccuracy: 58.86\n",
            "\tEpoch : 295 \tLoss: 0.668 \tAccuracy: 58.86\n",
            "\tEpoch : 296 \tLoss: 0.667 \tAccuracy: 58.86\n",
            "\tEpoch : 297 \tLoss: 0.664 \tAccuracy: 58.86\n",
            "\tEpoch : 298 \tLoss: 0.661 \tAccuracy: 58.86\n",
            "\tEpoch : 299 \tLoss: 0.657 \tAccuracy: 58.86\n",
            "\tEpoch : 300 \tLoss: 0.651 \tAccuracy: 58.86\n",
            "\tEpoch : 301 \tLoss: 0.643 \tAccuracy: 58.86\n",
            "\tEpoch : 302 \tLoss: 0.631 \tAccuracy: 58.86\n",
            "\tEpoch : 303 \tLoss: 0.612 \tAccuracy: 58.86\n",
            "\tEpoch : 304 \tLoss: 0.584 \tAccuracy: 58.86\n",
            "\tEpoch : 305 \tLoss: 0.546 \tAccuracy: 58.86\n",
            "\tEpoch : 306 \tLoss: 0.509 \tAccuracy: 58.86\n",
            "\tEpoch : 307 \tLoss: 0.46 \tAccuracy: 58.86\n",
            "\tEpoch : 308 \tLoss: 0.422 \tAccuracy: 85.71\n",
            "\tEpoch : 309 \tLoss: 0.404 \tAccuracy: 94.43\n",
            "\tEpoch : 310 \tLoss: 0.37 \tAccuracy: 90.43\n",
            "\tEpoch : 311 \tLoss: 0.352 \tAccuracy: 93.14\n",
            "\tEpoch : 312 \tLoss: 0.338 \tAccuracy: 93.14\n",
            "\tEpoch : 313 \tLoss: 0.327 \tAccuracy: 93.57\n",
            "\tEpoch : 314 \tLoss: 0.317 \tAccuracy: 94.43\n",
            "\tEpoch : 315 \tLoss: 0.309 \tAccuracy: 94.57\n",
            "\tEpoch : 316 \tLoss: 0.302 \tAccuracy: 94.43\n",
            "\tEpoch : 317 \tLoss: 0.296 \tAccuracy: 94.43\n",
            "\tEpoch : 318 \tLoss: 0.291 \tAccuracy: 94.43\n",
            "\tEpoch : 319 \tLoss: 0.287 \tAccuracy: 94.86\n",
            "\tEpoch : 320 \tLoss: 0.283 \tAccuracy: 95.0\n",
            "\tEpoch : 321 \tLoss: 0.28 \tAccuracy: 95.0\n",
            "\tEpoch : 322 \tLoss: 0.277 \tAccuracy: 95.0\n",
            "\tEpoch : 323 \tLoss: 0.274 \tAccuracy: 95.0\n",
            "\tEpoch : 324 \tLoss: 0.272 \tAccuracy: 95.0\n",
            "\tEpoch : 325 \tLoss: 0.27 \tAccuracy: 95.0\n",
            "\tEpoch : 326 \tLoss: 0.268 \tAccuracy: 95.0\n",
            "\tEpoch : 327 \tLoss: 0.266 \tAccuracy: 94.86\n",
            "\tEpoch : 328 \tLoss: 0.265 \tAccuracy: 94.86\n",
            "\tEpoch : 329 \tLoss: 0.263 \tAccuracy: 94.86\n",
            "\tEpoch : 330 \tLoss: 0.262 \tAccuracy: 94.86\n",
            "\tEpoch : 331 \tLoss: 0.261 \tAccuracy: 94.86\n",
            "\tEpoch : 332 \tLoss: 0.26 \tAccuracy: 94.57\n",
            "\tEpoch : 333 \tLoss: 0.259 \tAccuracy: 94.71\n",
            "\tEpoch : 334 \tLoss: 0.258 \tAccuracy: 94.57\n",
            "\tEpoch : 335 \tLoss: 0.257 \tAccuracy: 94.57\n",
            "\tEpoch : 336 \tLoss: 0.256 \tAccuracy: 94.43\n",
            "\tEpoch : 337 \tLoss: 0.256 \tAccuracy: 94.57\n",
            "\tEpoch : 338 \tLoss: 0.255 \tAccuracy: 94.43\n",
            "\tEpoch : 339 \tLoss: 0.255 \tAccuracy: 94.57\n",
            "\tEpoch : 340 \tLoss: 0.254 \tAccuracy: 94.43\n",
            "\tEpoch : 341 \tLoss: 0.253 \tAccuracy: 94.71\n",
            "\tEpoch : 342 \tLoss: 0.253 \tAccuracy: 94.71\n",
            "\tEpoch : 343 \tLoss: 0.252 \tAccuracy: 94.71\n",
            "\tEpoch : 344 \tLoss: 0.252 \tAccuracy: 94.71\n",
            "\tEpoch : 345 \tLoss: 0.251 \tAccuracy: 94.57\n",
            "\tEpoch : 346 \tLoss: 0.251 \tAccuracy: 94.29\n",
            "\tEpoch : 347 \tLoss: 0.251 \tAccuracy: 94.71\n",
            "\tEpoch : 348 \tLoss: 0.251 \tAccuracy: 94.29\n",
            "\tEpoch : 349 \tLoss: 0.25 \tAccuracy: 94.71\n",
            "\tEpoch : 350 \tLoss: 0.25 \tAccuracy: 94.29\n",
            "\tEpoch : 351 \tLoss: 0.249 \tAccuracy: 94.71\n",
            "\tEpoch : 352 \tLoss: 0.249 \tAccuracy: 94.29\n",
            "\tEpoch : 353 \tLoss: 0.249 \tAccuracy: 94.71\n",
            "\tEpoch : 354 \tLoss: 0.248 \tAccuracy: 94.43\n",
            "\tEpoch : 355 \tLoss: 0.248 \tAccuracy: 94.71\n",
            "\tEpoch : 356 \tLoss: 0.248 \tAccuracy: 94.43\n",
            "\tEpoch : 357 \tLoss: 0.248 \tAccuracy: 94.71\n",
            "\tEpoch : 358 \tLoss: 0.249 \tAccuracy: 94.57\n",
            "\tEpoch : 359 \tLoss: 0.248 \tAccuracy: 94.57\n",
            "\tEpoch : 360 \tLoss: 0.248 \tAccuracy: 94.57\n",
            "\tEpoch : 361 \tLoss: 0.248 \tAccuracy: 94.57\n",
            "\tEpoch : 362 \tLoss: 0.248 \tAccuracy: 94.29\n",
            "\tEpoch : 363 \tLoss: 0.248 \tAccuracy: 94.57\n",
            "\tEpoch : 364 \tLoss: 0.251 \tAccuracy: 93.0\n",
            "\tEpoch : 365 \tLoss: 0.25 \tAccuracy: 93.86\n",
            "\tEpoch : 366 \tLoss: 0.252 \tAccuracy: 92.57\n",
            "\tEpoch : 367 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 368 \tLoss: 0.253 \tAccuracy: 92.14\n",
            "\tEpoch : 369 \tLoss: 0.253 \tAccuracy: 93.71\n",
            "\tEpoch : 370 \tLoss: 0.258 \tAccuracy: 91.29\n",
            "\tEpoch : 371 \tLoss: 0.257 \tAccuracy: 93.29\n",
            "\tEpoch : 372 \tLoss: 0.265 \tAccuracy: 91.0\n",
            "\tEpoch : 373 \tLoss: 0.262 \tAccuracy: 92.71\n",
            "\tEpoch : 374 \tLoss: 0.266 \tAccuracy: 90.43\n",
            "\tEpoch : 375 \tLoss: 0.262 \tAccuracy: 92.71\n",
            "\tEpoch : 376 \tLoss: 0.269 \tAccuracy: 90.14\n",
            "\tEpoch : 377 \tLoss: 0.262 \tAccuracy: 92.86\n",
            "\tEpoch : 378 \tLoss: 0.264 \tAccuracy: 90.86\n",
            "\tEpoch : 379 \tLoss: 0.259 \tAccuracy: 93.29\n",
            "\tEpoch : 380 \tLoss: 0.262 \tAccuracy: 91.0\n",
            "\tEpoch : 381 \tLoss: 0.257 \tAccuracy: 93.43\n",
            "\tEpoch : 382 \tLoss: 0.258 \tAccuracy: 91.29\n",
            "\tEpoch : 383 \tLoss: 0.255 \tAccuracy: 93.57\n",
            "\tEpoch : 384 \tLoss: 0.257 \tAccuracy: 91.29\n",
            "\tEpoch : 385 \tLoss: 0.254 \tAccuracy: 93.57\n",
            "\tEpoch : 386 \tLoss: 0.255 \tAccuracy: 91.43\n",
            "\tEpoch : 387 \tLoss: 0.252 \tAccuracy: 93.71\n",
            "\tEpoch : 388 \tLoss: 0.253 \tAccuracy: 91.71\n",
            "\tEpoch : 389 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 390 \tLoss: 0.251 \tAccuracy: 92.0\n",
            "\tEpoch : 391 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 392 \tLoss: 0.25 \tAccuracy: 92.29\n",
            "\tEpoch : 393 \tLoss: 0.249 \tAccuracy: 93.86\n",
            "\tEpoch : 394 \tLoss: 0.25 \tAccuracy: 92.43\n",
            "\tEpoch : 395 \tLoss: 0.248 \tAccuracy: 93.86\n",
            "\tEpoch : 396 \tLoss: 0.249 \tAccuracy: 92.71\n",
            "\tEpoch : 397 \tLoss: 0.248 \tAccuracy: 93.86\n",
            "\tEpoch : 398 \tLoss: 0.248 \tAccuracy: 93.0\n",
            "\tEpoch : 399 \tLoss: 0.247 \tAccuracy: 94.14\n",
            "\tEpoch : 400 \tLoss: 0.248 \tAccuracy: 93.0\n",
            "\tEpoch : 401 \tLoss: 0.247 \tAccuracy: 94.14\n",
            "\tEpoch : 402 \tLoss: 0.248 \tAccuracy: 93.0\n",
            "\tEpoch : 403 \tLoss: 0.248 \tAccuracy: 93.86\n",
            "\tEpoch : 404 \tLoss: 0.249 \tAccuracy: 92.29\n",
            "\tEpoch : 405 \tLoss: 0.249 \tAccuracy: 93.71\n",
            "\tEpoch : 406 \tLoss: 0.25 \tAccuracy: 92.14\n",
            "\tEpoch : 407 \tLoss: 0.249 \tAccuracy: 93.71\n",
            "\tEpoch : 408 \tLoss: 0.251 \tAccuracy: 91.86\n",
            "\tEpoch : 409 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 410 \tLoss: 0.253 \tAccuracy: 91.43\n",
            "\tEpoch : 411 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 412 \tLoss: 0.252 \tAccuracy: 91.57\n",
            "\tEpoch : 413 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 414 \tLoss: 0.253 \tAccuracy: 91.43\n",
            "\tEpoch : 415 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 416 \tLoss: 0.252 \tAccuracy: 91.43\n",
            "\tEpoch : 417 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 418 \tLoss: 0.252 \tAccuracy: 91.57\n",
            "\tEpoch : 419 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 420 \tLoss: 0.252 \tAccuracy: 91.71\n",
            "\tEpoch : 421 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 422 \tLoss: 0.252 \tAccuracy: 91.57\n",
            "\tEpoch : 423 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 424 \tLoss: 0.252 \tAccuracy: 91.43\n",
            "\tEpoch : 425 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 426 \tLoss: 0.253 \tAccuracy: 91.43\n",
            "\tEpoch : 427 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 428 \tLoss: 0.253 \tAccuracy: 91.43\n",
            "\tEpoch : 429 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 430 \tLoss: 0.253 \tAccuracy: 91.43\n",
            "\tEpoch : 431 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 432 \tLoss: 0.252 \tAccuracy: 91.57\n",
            "\tEpoch : 433 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 434 \tLoss: 0.252 \tAccuracy: 91.43\n",
            "\tEpoch : 435 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 436 \tLoss: 0.251 \tAccuracy: 91.71\n",
            "\tEpoch : 437 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 438 \tLoss: 0.25 \tAccuracy: 91.86\n",
            "\tEpoch : 439 \tLoss: 0.25 \tAccuracy: 93.86\n",
            "\tEpoch : 440 \tLoss: 0.251 \tAccuracy: 91.71\n",
            "\tEpoch : 441 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 442 \tLoss: 0.252 \tAccuracy: 91.57\n",
            "\tEpoch : 443 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 444 \tLoss: 0.253 \tAccuracy: 91.43\n",
            "\tEpoch : 445 \tLoss: 0.252 \tAccuracy: 93.71\n",
            "\tEpoch : 446 \tLoss: 0.253 \tAccuracy: 91.29\n",
            "\tEpoch : 447 \tLoss: 0.252 \tAccuracy: 93.57\n",
            "\tEpoch : 448 \tLoss: 0.254 \tAccuracy: 91.29\n",
            "\tEpoch : 449 \tLoss: 0.253 \tAccuracy: 93.57\n",
            "\tEpoch : 450 \tLoss: 0.255 \tAccuracy: 91.29\n",
            "\tEpoch : 451 \tLoss: 0.253 \tAccuracy: 93.57\n",
            "\tEpoch : 452 \tLoss: 0.255 \tAccuracy: 91.29\n",
            "\tEpoch : 453 \tLoss: 0.253 \tAccuracy: 93.57\n",
            "\tEpoch : 454 \tLoss: 0.255 \tAccuracy: 91.29\n",
            "\tEpoch : 455 \tLoss: 0.253 \tAccuracy: 93.57\n",
            "\tEpoch : 456 \tLoss: 0.254 \tAccuracy: 91.29\n",
            "\tEpoch : 457 \tLoss: 0.253 \tAccuracy: 93.57\n",
            "\tEpoch : 458 \tLoss: 0.254 \tAccuracy: 91.29\n",
            "\tEpoch : 459 \tLoss: 0.252 \tAccuracy: 93.71\n",
            "\tEpoch : 460 \tLoss: 0.253 \tAccuracy: 91.29\n",
            "\tEpoch : 461 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 462 \tLoss: 0.252 \tAccuracy: 91.43\n",
            "\tEpoch : 463 \tLoss: 0.251 \tAccuracy: 93.71\n",
            "\tEpoch : 464 \tLoss: 0.251 \tAccuracy: 91.57\n",
            "\tEpoch : 465 \tLoss: 0.25 \tAccuracy: 93.71\n",
            "\tEpoch : 466 \tLoss: 0.251 \tAccuracy: 91.71\n",
            "\tEpoch : 467 \tLoss: 0.25 \tAccuracy: 93.86\n",
            "\tEpoch : 468 \tLoss: 0.25 \tAccuracy: 91.71\n",
            "\tEpoch : 469 \tLoss: 0.249 \tAccuracy: 93.71\n",
            "\tEpoch : 470 \tLoss: 0.249 \tAccuracy: 91.86\n",
            "\tEpoch : 471 \tLoss: 0.248 \tAccuracy: 93.57\n",
            "\tEpoch : 472 \tLoss: 0.249 \tAccuracy: 91.86\n",
            "\tEpoch : 473 \tLoss: 0.248 \tAccuracy: 93.57\n",
            "\tEpoch : 474 \tLoss: 0.248 \tAccuracy: 92.14\n",
            "\tEpoch : 475 \tLoss: 0.248 \tAccuracy: 93.57\n",
            "\tEpoch : 476 \tLoss: 0.248 \tAccuracy: 92.29\n",
            "\tEpoch : 477 \tLoss: 0.247 \tAccuracy: 93.71\n",
            "\tEpoch : 478 \tLoss: 0.248 \tAccuracy: 92.29\n",
            "\tEpoch : 479 \tLoss: 0.247 \tAccuracy: 93.86\n",
            "\tEpoch : 480 \tLoss: 0.247 \tAccuracy: 92.29\n",
            "\tEpoch : 481 \tLoss: 0.246 \tAccuracy: 94.14\n",
            "\tEpoch : 482 \tLoss: 0.247 \tAccuracy: 92.57\n",
            "\tEpoch : 483 \tLoss: 0.246 \tAccuracy: 94.14\n",
            "\tEpoch : 484 \tLoss: 0.246 \tAccuracy: 92.71\n",
            "\tEpoch : 485 \tLoss: 0.246 \tAccuracy: 94.29\n",
            "\tEpoch : 486 \tLoss: 0.246 \tAccuracy: 93.0\n",
            "\tEpoch : 487 \tLoss: 0.246 \tAccuracy: 94.29\n",
            "\tEpoch : 488 \tLoss: 0.246 \tAccuracy: 93.0\n",
            "\tEpoch : 489 \tLoss: 0.246 \tAccuracy: 94.29\n",
            "\tEpoch : 490 \tLoss: 0.247 \tAccuracy: 92.71\n",
            "\tEpoch : 491 \tLoss: 0.246 \tAccuracy: 94.0\n",
            "\tEpoch : 492 \tLoss: 0.247 \tAccuracy: 92.29\n",
            "\tEpoch : 493 \tLoss: 0.247 \tAccuracy: 93.86\n",
            "\tEpoch : 494 \tLoss: 0.247 \tAccuracy: 92.29\n",
            "\tEpoch : 495 \tLoss: 0.247 \tAccuracy: 93.71\n",
            "\tEpoch : 496 \tLoss: 0.248 \tAccuracy: 92.29\n",
            "\tEpoch : 497 \tLoss: 0.248 \tAccuracy: 93.57\n",
            "\tEpoch : 498 \tLoss: 0.248 \tAccuracy: 91.86\n",
            "\tEpoch : 499 \tLoss: 0.248 \tAccuracy: 93.57\n",
            "\tEpoch : 500 \tLoss: 0.249 \tAccuracy: 91.86\n",
            "\n",
            "Testing our Deep Neural Network:\n",
            "\tLoss: 0.33 \tAccuracy: 93.33\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}