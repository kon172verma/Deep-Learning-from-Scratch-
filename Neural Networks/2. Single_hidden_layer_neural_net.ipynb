{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Single_hidden_layer_neural_net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4z2CUKN2HMo",
        "colab_type": "text"
      },
      "source": [
        "<h2><b>Single Hidden Layer Neural Network using Numpy</b></h2>\n",
        "<p>-Konark Verma\n",
        "<p>We wish to implement a single Hidden Layer Neural Network, from scratch just using Numpy and test this on a given dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djD4TsTo0pyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the required libraries.\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wuuOFc1EFym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "257c02af-fd4a-4223-d56d-48b088a35134"
      },
      "source": [
        "# Preprocessing the training and testing data.\n",
        "data = np.genfromtxt('Dataset.csv', delimiter=',')\n",
        "data_x, data_y = preprocessing.scale(data[:,:5]), data[:,5]\n",
        "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.3, random_state=1)\n",
        "train_x, train_y, test_x, test_y = train_x.T, train_y.reshape(1,len(train_y)), test_x.T, test_y.reshape(1,len(test_y))\n",
        "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5, 700), (1, 700), (5, 300), (1, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2IXIW7t1CDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for sigmoid, relu, initializing parameters, computing loss and accuracy.\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.where(z>0, z, 0)\n",
        "\n",
        "def relu_der(z):\n",
        "    return np.where(z>0, 1, 0)\n",
        "\n",
        "def initialize_parameters(n,n1):\n",
        "    w1 = np.random.rand(n1,n) * 0.01\n",
        "    b1 = np.zeros((n1,1))\n",
        "    w2 = np.random.rand(1,n1) * 0.01\n",
        "    b2 = np.zeros((1,1))\n",
        "    return w1, b1, w2, b2\n",
        "\n",
        "def compute_loss(Y, a2, m):\n",
        "    loss = -np.sum(Y*np.log(a2) + (1-Y)*np.log(1-a2))/m\n",
        "    return loss\n",
        "\n",
        "def compute_accuracy(Y, a2, m):\n",
        "     accuracy = 1-np.sum((a2+0.5).astype(int) ^ Y.astype(int))/m\n",
        "     return accuracy*100\n",
        "\n",
        "def forward_propogation(X, Y, m, w1, b1, w2, b2):\n",
        "    z1 = np.dot(w1,X) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(w2,a1) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    loss = compute_loss(Y, a2, m)\n",
        "    accuracy = compute_accuracy(Y, a2, m)\n",
        "    cache = (z1, a1)\n",
        "    return a2, cache, loss, accuracy \n",
        "\n",
        "def backward_propogation(X, Y, m, a2, cache, w1, b1, w2, b2, learning_rate):\n",
        "    z1, a1 = cache\n",
        "    dz2 = a2-Y\n",
        "    dw2 = np.dot(dz2, a1.T)/m\n",
        "    db2 = np.sum(dz2)/m\n",
        "    dz1 = np.dot(w2.T,dz2) * relu_der(z1)\n",
        "    dw1 = np.dot(dz1,X.T)/m\n",
        "    db1 = np.sum(dz1, axis=1, keepdims=True)/m\n",
        "    w2 = w2 - learning_rate*dw2\n",
        "    b2 = b2 - learning_rate*db2\n",
        "    w1 = w1 - learning_rate*dw1\n",
        "    b1 = b1 - learning_rate*db1\n",
        "    return w1, b1, w2, b2\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dZ25ILS1Ycg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for our single layer neural network.\n",
        "def Neural_network(X, Y, X_test, Y_test, n1, learning_rate=1, epochs=100):\n",
        "    n, m = X.shape\n",
        "    w1, b1, w2, b2 = initialize_parameters(n, n1)\n",
        "    print('\\nTraining the Neural Network:\\n')\n",
        "    for epoch in range(epochs):\n",
        "        a2, cache, loss, accuracy = forward_propogation(X, Y, m, w1, b1, w2, b2)\n",
        "        print('\\tEpoch :',epoch+1, 'Loss :',loss, 'Accuracy :',accuracy)\n",
        "        w1, b1, w2, b2 = backward_propogation(X, Y, m, a2, cache, w1, b1, w2, b2, learning_rate)\n",
        "    print('\\nTesting the Neural Network:\\n')\n",
        "    _, _, loss, accuracy = forward_propogation(X_test, Y_test, Y_test.shape[1], w1, b1, w2, b2)\n",
        "    print('\\tLoss :',loss, 'Accuracy :',accuracy)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNboEoLqEZep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "48148607-b06c-4808-9d6c-61fe28676fe4"
      },
      "source": [
        "# Training and testing our neural network on our dataset.\n",
        "n1 = 32\n",
        "Neural_network(train_x, train_y, test_x, test_y, n1, learning_rate=3, epochs=50)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training the Neural Network:\n",
            "\n",
            "\tEpoch : 1 Loss : 0.6929894419641867 Accuracy : 58.857142857142854\n",
            "\tEpoch : 2 Loss : 0.6780014204696383 Accuracy : 58.857142857142854\n",
            "\tEpoch : 3 Loss : 0.6762818532897524 Accuracy : 58.857142857142854\n",
            "\tEpoch : 4 Loss : 0.673991808215724 Accuracy : 58.857142857142854\n",
            "\tEpoch : 5 Loss : 0.6673719922391487 Accuracy : 58.857142857142854\n",
            "\tEpoch : 6 Loss : 0.6484969984032052 Accuracy : 58.857142857142854\n",
            "\tEpoch : 7 Loss : 0.6020473361412072 Accuracy : 58.857142857142854\n",
            "\tEpoch : 8 Loss : 0.5216445966703323 Accuracy : 58.857142857142854\n",
            "\tEpoch : 9 Loss : 0.4394576502672246 Accuracy : 87.57142857142857\n",
            "\tEpoch : 10 Loss : 0.3797727437283845 Accuracy : 91.57142857142857\n",
            "\tEpoch : 11 Loss : 0.3404105367107884 Accuracy : 93.42857142857143\n",
            "\tEpoch : 12 Loss : 0.31451582827874736 Accuracy : 94.42857142857143\n",
            "\tEpoch : 13 Loss : 0.29710421423839395 Accuracy : 94.71428571428572\n",
            "\tEpoch : 14 Loss : 0.28505472053363745 Accuracy : 94.71428571428572\n",
            "\tEpoch : 15 Loss : 0.27639486864788365 Accuracy : 94.71428571428572\n",
            "\tEpoch : 16 Loss : 0.2700354467052444 Accuracy : 94.71428571428572\n",
            "\tEpoch : 17 Loss : 0.26529510488527797 Accuracy : 94.57142857142857\n",
            "\tEpoch : 18 Loss : 0.2616712895582941 Accuracy : 94.42857142857143\n",
            "\tEpoch : 19 Loss : 0.2588362082511927 Accuracy : 94.57142857142857\n",
            "\tEpoch : 20 Loss : 0.25648385128899737 Accuracy : 94.71428571428572\n",
            "\tEpoch : 21 Loss : 0.2545441306587105 Accuracy : 94.71428571428572\n",
            "\tEpoch : 22 Loss : 0.2529019077037424 Accuracy : 94.42857142857143\n",
            "\tEpoch : 23 Loss : 0.25152032250054446 Accuracy : 94.42857142857143\n",
            "\tEpoch : 24 Loss : 0.2503447481621209 Accuracy : 94.42857142857143\n",
            "\tEpoch : 25 Loss : 0.24935746073638296 Accuracy : 94.42857142857143\n",
            "\tEpoch : 26 Loss : 0.24853000545382964 Accuracy : 94.42857142857143\n",
            "\tEpoch : 27 Loss : 0.24782843007507824 Accuracy : 94.42857142857143\n",
            "\tEpoch : 28 Loss : 0.24722747492890512 Accuracy : 94.42857142857143\n",
            "\tEpoch : 29 Loss : 0.24670442212507973 Accuracy : 94.42857142857143\n",
            "\tEpoch : 30 Loss : 0.2462255524005113 Accuracy : 94.42857142857143\n",
            "\tEpoch : 31 Loss : 0.24580837699560903 Accuracy : 94.42857142857143\n",
            "\tEpoch : 32 Loss : 0.2454434617353274 Accuracy : 94.42857142857143\n",
            "\tEpoch : 33 Loss : 0.24512036989624172 Accuracy : 94.42857142857143\n",
            "\tEpoch : 34 Loss : 0.24483659836481536 Accuracy : 94.57142857142857\n",
            "\tEpoch : 35 Loss : 0.2445890517763477 Accuracy : 94.57142857142857\n",
            "\tEpoch : 36 Loss : 0.24435830625445248 Accuracy : 94.57142857142857\n",
            "\tEpoch : 37 Loss : 0.24409455926522558 Accuracy : 94.57142857142857\n",
            "\tEpoch : 38 Loss : 0.24385999684585885 Accuracy : 94.57142857142857\n",
            "\tEpoch : 39 Loss : 0.24364217726438783 Accuracy : 94.57142857142857\n",
            "\tEpoch : 40 Loss : 0.24345196977117955 Accuracy : 94.57142857142857\n",
            "\tEpoch : 41 Loss : 0.2432833739909901 Accuracy : 94.57142857142857\n",
            "\tEpoch : 42 Loss : 0.2431348995630086 Accuracy : 94.57142857142857\n",
            "\tEpoch : 43 Loss : 0.24300302189396514 Accuracy : 94.57142857142857\n",
            "\tEpoch : 44 Loss : 0.24287750251849746 Accuracy : 94.57142857142857\n",
            "\tEpoch : 45 Loss : 0.2427652374572924 Accuracy : 94.57142857142857\n",
            "\tEpoch : 46 Loss : 0.24266407610884147 Accuracy : 94.57142857142857\n",
            "\tEpoch : 47 Loss : 0.2425468417287575 Accuracy : 94.57142857142857\n",
            "\tEpoch : 48 Loss : 0.2424187447459094 Accuracy : 94.57142857142857\n",
            "\tEpoch : 49 Loss : 0.2423121637557129 Accuracy : 94.57142857142857\n",
            "\tEpoch : 50 Loss : 0.24221721723317244 Accuracy : 94.57142857142857\n",
            "\n",
            "Testing the Neural Network:\n",
            "\n",
            "\tLoss : 0.3139044779580065 Accuracy : 94.33333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}